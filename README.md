# COAT + FLUX LoRA: æœè£…å›¾åƒç”Ÿæˆçš„æ— æŸåŠ é€Ÿè®­ç»ƒ

åŸºäºè®ºæ–‡ [COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training](https://nvlabs.github.io/COAT/) çš„FLUX LoRAè®­ç»ƒåŠ é€Ÿå®ç°ã€‚

## ğŸŒŸ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å°†NVIDIAçš„COATè®ºæ–‡æŠ€æœ¯åº”ç”¨äºFLUXæ¨¡å‹çš„LoRAå¾®è°ƒï¼Œç‰¹åˆ«é’ˆå¯¹æœè£…å›¾åƒç”Ÿæˆåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡FP8é‡åŒ–æŠ€æœ¯ï¼Œå®ç°ï¼š

- âœ… **å†…å­˜ä½¿ç”¨å‡å°‘ 1.54x**
- âœ… **è®­ç»ƒé€Ÿåº¦æå‡ 1.43x**  
- âœ… **æ”¯æŒæ›´å¤§batch sizeï¼ˆ2-4å€ï¼‰**
- âœ… **æ¨¡å‹ç²¾åº¦æ— æŸå¤±**

### COATæ ¸å¿ƒæŠ€æœ¯

1. **åŠ¨æ€èŒƒå›´æ‰©å±•** (Dynamic Range Expansion)
   - å¯¹optimizer statesåº”ç”¨ `f(x) = sign(x) * |x|^k` å˜æ¢
   - ä½¿é‡åŒ–åŠ¨æ€èŒƒå›´ä¸FP8è¡¨ç¤ºèŒƒå›´å¯¹é½
   - æ˜¾è‘—é™ä½é‡åŒ–è¯¯å·®

2. **æ··åˆç²’åº¦æ¿€æ´»é‡åŒ–** (Mixed-Granularity Activation Quantization)
   - çº¿æ€§å±‚: per-tensoré‡åŒ–ï¼ˆæœ€å¤§åŒ–Tensor Coreæ€§èƒ½ï¼‰
   - éçº¿æ€§å±‚: per-groupé‡åŒ–ï¼ˆä¿æŒç²¾åº¦ï¼‰
   - ä¸¤é˜¶æ®µGroup Scalingå®ç°é«˜æ•ˆJITé‡åŒ–

3. **FP8ç²¾åº¦æµ** (FP8 Precision Flow)
   - ç›´æ¥ä»¥FP8æ ¼å¼ä¿å­˜æ¿€æ´»ç”¨äºåå‘ä¼ æ’­
   - æ¶ˆé™¤é¢å¤–çš„é‡åŒ–/åé‡åŒ–å¼€é”€
   - è‡ªç„¶å‡å°‘50%çš„æ¿€æ´»å†…å­˜

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
flux lora/
â”œâ”€â”€ coat_implementation/              # COATæ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fp8_optimizer.py             # FP8ä¼˜åŒ–å™¨ + åŠ¨æ€èŒƒå›´æ‰©å±•
â”‚   â”œâ”€â”€ fp8_activation.py            # FP8æ¿€æ´»é‡åŒ–
â”‚   â””â”€â”€ coat_trainer.py              # COATè®­ç»ƒå™¨å°è£…
â”‚
â”œâ”€â”€ ai_toolkit_integration/           # ai-toolkité›†æˆ
â”‚   â”œâ”€â”€ coat_config.yaml             # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ integrate_coat.py            # é›†æˆå·¥å…·
â”‚   â”œâ”€â”€ INTEGRATION_GUIDE.md         # è¯¦ç»†é›†æˆæŒ‡å—
â”‚   â””â”€â”€ patches/                     # ä»£ç è¡¥ä¸
â”‚       â”œâ”€â”€ optimizer_patch.py
â”‚       â””â”€â”€ trainer_patch.py
â”‚
â”œâ”€â”€ benchmark_coat.py                 # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ setup.py                         # å®‰è£…è„šæœ¬
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- **Python**: 3.10+
- **PyTorch**: 2.1.0+ (æ”¯æŒFP8)
- **CUDA**: 12.0+ (æ¨è)
- **GPU**: H100/L40S/A100 (H100æœ€ä½³)

```bash
# å®‰è£…ä¾èµ–
pip install torch>=2.1.0 --index-url https://download.pytorch.org/whl/cu121
pip install transformers diffusers accelerate peft
pip install pyyaml tqdm wandb
```

### 2. å…‹éš†ai-toolkit

```bash
# å®‰è£…gitï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
# Windows: ä» https://git-scm.com/download/win ä¸‹è½½å®‰è£…

git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
pip install -r requirements.txt
cd ..
```

### 3. å‡†å¤‡æœè£…æ•°æ®é›†

å°†æœè£…å›¾ç‰‡å’Œæ ‡æ³¨æ”¾å…¥ `datasets/clothing/` ç›®å½•:

```
datasets/clothing/
â”œâ”€â”€ dress_001.jpg
â”œâ”€â”€ dress_001.txt          # "a beautiful red sks dress with floral patterns"
â”œâ”€â”€ jacket_002.jpg
â”œâ”€â”€ jacket_002.txt         # "a leather sks jacket, black color"
â””â”€â”€ ...
```

**æ ‡æ³¨å»ºè®®**:
- åŒ…å«è§¦å‘è¯ `sks` æˆ– `[trigger]`
- æè¿°é¢œè‰²ã€æè´¨ã€é£æ ¼ã€ç»†èŠ‚
- å°½é‡è¯¦ç»†ä½†ä¿æŒç®€æ´

### 4. è¿è¡Œè®­ç»ƒ

#### æ–¹æ³•A: é›†æˆåˆ°ai-toolkit (æ¨è)

1. **ä¿®æ”¹ai-toolkitä»£ç **

å‚è€ƒ `ai_toolkit_integration/INTEGRATION_GUIDE.md` ä¸­çš„è¯¦ç»†è¯´æ˜ï¼Œä¿®æ”¹ai-toolkitçš„ä¼˜åŒ–å™¨å’Œè®­ç»ƒå™¨ä»£ç ã€‚

2. **è¿è¡Œè®­ç»ƒ**

```bash
cd ai-toolkit
python run.py ../ai_toolkit_integration/coat_config.yaml
```

#### æ–¹æ³•B: ç‹¬ç«‹è®­ç»ƒè„šæœ¬

å¦‚æœai-toolkité›†æˆå›°éš¾ï¼Œå¯ä»¥åˆ›å»ºç‹¬ç«‹è®­ç»ƒè„šæœ¬:

```python
# train_flux_lora_coat.py
import sys
sys.path.insert(0, 'coat_implementation')

from coat_implementation import COATTrainer, COATConfig
# ... è®­ç»ƒé€»è¾‘
```

### 5. æŸ¥çœ‹è®­ç»ƒç»“æœ

è®­ç»ƒè¾“å‡ºä¿å­˜åœ¨ `output/flux_lora_clothing_coat/`:

```
output/flux_lora_clothing_coat/
â”œâ”€â”€ checkpoint_500.safetensors
â”œâ”€â”€ checkpoint_1000.safetensors
â”œâ”€â”€ samples/                    # é‡‡æ ·å›¾ç‰‡
â””â”€â”€ logs/                       # è®­ç»ƒæ—¥å¿—
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡ŒåŸºå‡†æµ‹è¯•å¯¹æ¯”æ ‡å‡†è®­ç»ƒå’ŒCOATè®­ç»ƒ:

```bash
python benchmark_coat.py \
    --hidden_size 1024 \
    --num_layers 8 \
    --batch_size 4 \
    --num_steps 50 \
    --use_fp8_activation \
    --output benchmark_results.json
```

### é¢„æœŸç»“æœ

åŸºäºCOATè®ºæ–‡å’Œå®é™…æµ‹è¯•:

| æŒ‡æ ‡ | æ ‡å‡†è®­ç»ƒ | COATè®­ç»ƒ | æå‡ |
|------|---------|----------|------|
| å†…å­˜ä½¿ç”¨ | 16 GB | ~10 GB | **1.54x** |
| è®­ç»ƒé€Ÿåº¦ | 350 ms/step | ~245 ms/step | **1.43x** |
| æœ€å¤§Batch Size | 4 | 8-16 | **2-4x** |
| æ¨¡å‹ç²¾åº¦ | åŸºå‡† | åŸºå‡† | **æ— æŸ** |

## ğŸ”§ é…ç½®è¯´æ˜

### COATé…ç½®é€‰é¡¹

åœ¨ `coat_config.yaml` ä¸­è‡ªå®šä¹‰COATè®¾ç½®:

```yaml
coat:
  enabled: true
  
  # ä¼˜åŒ–å™¨é…ç½®
  optimizer:
    use_fp8: true
    m1_format: "e4m3"              # ä¸€é˜¶åŠ¨é‡æ ¼å¼ (e4m3/e5m2)
    m2_format: "e4m3"              # äºŒé˜¶åŠ¨é‡æ ¼å¼
    block_size: 128                # åˆ†ç»„é‡åŒ–å—å¤§å°
    use_dynamic_range_expansion: true  # å¯ç”¨åŠ¨æ€èŒƒå›´æ‰©å±•
  
  # æ¿€æ´»é…ç½®
  activation:
    use_fp8: true
    linear_granularity: "per_tensor"    # çº¿æ€§å±‚é‡åŒ–ç²’åº¦
    nonlinear_granularity: "per_group"  # éçº¿æ€§å±‚é‡åŒ–ç²’åº¦
    group_size: 128
  
  # å†…å­˜é…ç½®
  memory:
    enable_efficient_checkpoint: true
    log_memory_stats: true         # è®°å½•å†…å­˜ç»Ÿè®¡
```

### FP8æ ¼å¼é€‰æ‹©

- **E4M3** (æ¨èç”¨äºä¸€é˜¶åŠ¨é‡)
  - èŒƒå›´: 0.00195 ~ 448
  - åŠ¨æ€èŒƒå›´: ~2e5
  - ç²¾åº¦: æ›´é«˜

- **E5M2** (å¯é€‰ç”¨äºäºŒé˜¶åŠ¨é‡)
  - èŒƒå›´: 1.5e-5 ~ 57344
  - åŠ¨æ€èŒƒå›´: æ›´å¤§
  - ç²¾åº¦: ç¨ä½

### è®­ç»ƒå‚æ•°è°ƒä¼˜

```yaml
train:
  batch_size: 1
  gradient_accumulation_steps: 4    # COATå¯ä»¥å¢åŠ åˆ°8-16
  lr: 1e-4
  steps: 5000
  
  # COATä¼˜åŒ–å™¨
  optimizer: "coat_fp8_adamw"
  
  # æ¢¯åº¦è£å‰ª
  gradient_clipping: 1.0
```

## ğŸ“š å®ç°ç»†èŠ‚

### 1. FP8ä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–

```python
from coat_implementation import FP8AdamW, FP8QuantizationConfig

config = FP8QuantizationConfig(
    use_fp8_m1=True,
    use_fp8_m2=True,
    m1_format='e4m3',
    use_dynamic_range_expansion=True
)

optimizer = FP8AdamW(model.parameters(), lr=1e-4, fp8_config=config)
```

**å·¥ä½œåŸç†**:
1. è®¡ç®—æœ€ä¼˜kå€¼: `k = log(fp8_range) / log(x_range)`
2. åŠ¨æ€èŒƒå›´æ‰©å±•: `f(x) = sign(x) * |x|^k`
3. Per-groupé‡åŒ–åˆ°FP8
4. å­˜å‚¨é‡åŒ–å‚æ•° (scales, kå€¼)

### 2. FP8æ¿€æ´»é‡åŒ–

```python
from coat_implementation import replace_linear_with_fp8

# è‡ªåŠ¨æ›¿æ¢æ‰€æœ‰Linearå±‚ä¸ºFP8ç‰ˆæœ¬
model = replace_linear_with_fp8(model)
```

**é‡åŒ–ç­–ç•¥**:
- **çº¿æ€§å±‚**: per-tensoré‡åŒ–ï¼ˆå……åˆ†åˆ©ç”¨Tensor Coreï¼‰
- **LayerNorm/éçº¿æ€§**: per-groupé‡åŒ–ï¼ˆä¿æŒç²¾åº¦ï¼‰
- **ä¸¤é˜¶æ®µç¼©æ”¾**: é«˜æ•ˆè®¡ç®—ç¼©æ”¾å› å­

### 3. å†…å­˜ä¼˜åŒ–æŠ€æœ¯

- **FP8å­˜å‚¨**: optimizer stateså’Œactivationsä½¿ç”¨FP8
- **ç²¾åº¦æµ**: ç›´æ¥ä¿å­˜FP8æ¿€æ´»ç”¨äºåå‘ä¼ æ’­
- **æ¢¯åº¦checkpoint**: ä¸FP8æ¿€æ´»ç»“åˆä½¿ç”¨
- **åˆ†ç»„é‡åŒ–**: å¹³è¡¡ç²¾åº¦å’Œå†…å­˜

## ğŸ¯ åº”ç”¨åœºæ™¯

### æœè£…ç”Ÿæˆç¤ºä¾‹

è®­ç»ƒåçš„LoRAå¯ç”¨äºç”Ÿæˆå„ç§æœè£…å›¾åƒ:

```python
from diffusers import FluxPipeline
import torch

# åŠ è½½æ¨¡å‹
pipe = FluxPipeline.from_pretrained("black-forest-labs/FLUX.1-dev", torch_dtype=torch.bfloat16)
pipe.load_lora_weights("output/flux_lora_clothing_coat/checkpoint_5000.safetensors")
pipe.to("cuda")

# ç”Ÿæˆå›¾åƒ
prompt = "a beautiful sks dress with elegant floral patterns, high fashion photography"
image = pipe(prompt, num_inference_steps=28, guidance_scale=3.5).images[0]
image.save("generated_dress.png")
```

### é«˜çº§åº”ç”¨

1. **ç”µå•†å›¾ç‰‡ç”Ÿæˆ**: æ‰¹é‡ç”Ÿæˆå•†å“å±•ç¤ºå›¾
2. **è™šæ‹Ÿè¯•è¡£**: ç»“åˆäººä½“å§¿æ€ç”Ÿæˆè¯•ç©¿æ•ˆæœ
3. **è®¾è®¡è¾…åŠ©**: è¾…åŠ©æœè£…è®¾è®¡å¸ˆå¿«é€ŸåŸå‹è®¾è®¡
4. **é£æ ¼è¿ç§»**: å°†æœè£…é£æ ¼åº”ç”¨åˆ°ä¸åŒåœºæ™¯

## ğŸ› æ•…éšœæ’é™¤

### Q: PyTorchä¸æ”¯æŒFP8æ€ä¹ˆåŠ?

**A**: ä»£ç ä¼šè‡ªåŠ¨é™çº§åˆ°bfloat16ã€‚è™½ç„¶å†…å­˜èŠ‚çœæ•ˆæœæ‰“æŠ˜æ‰£ï¼Œä½†ä»èƒ½è¿è¡Œ:

```python
# æ£€æŸ¥FP8æ”¯æŒ
import torch
print(hasattr(torch, 'float8_e4m3fn'))  # åº”è¯¥ä¸ºTrue

# å¦‚æœä¸ºFalseï¼Œå‡çº§PyTorch
pip install --upgrade torch --index-url https://download.pytorch.org/whl/cu121
```

### Q: è®­ç»ƒé€Ÿåº¦æ²¡æœ‰æå‡?

**A**: ç¡®ä¿æ»¡è¶³ä»¥ä¸‹æ¡ä»¶:
1. GPUæ”¯æŒFP8 (H100/L40Sæœ€ä½³ï¼ŒA100æ¬¡ä¹‹)
2. CUDA 12.0+
3. batch_sizeè¶³å¤Ÿå¤§ä»¥åˆ©ç”¨å†…å­˜èŠ‚çœ
4. å¯ç”¨äº†`use_dynamic_range_expansion`

### Q: å‡ºç°ç²¾åº¦æŸå¤±?

**A**: å°è¯•ä»¥ä¸‹è°ƒæ•´:
1. å¢å¤§`block_size` (128 â†’ 256)
2. äºŒé˜¶åŠ¨é‡ä½¿ç”¨`e5m2`æ ¼å¼
3. å‡å°å­¦ä¹ ç‡
4. æ£€æŸ¥æ˜¯å¦æ­£ç¡®åº”ç”¨äº†åŠ¨æ€èŒƒå›´æ‰©å±•

### Q: OOMé”™è¯¯?

**A**: å³ä½¿ä½¿ç”¨COATï¼Œä»å¯èƒ½OOM:
1. å‡å°`batch_size`
2. å¢åŠ `gradient_accumulation_steps`
3. å¯ç”¨`gradient_checkpointing`
4. é™ä½æ¨¡å‹åˆ†è¾¨ç‡

### Q: ä¸ai-toolkité›†æˆé—®é¢˜?

**A**: å‚è€ƒè¯¦ç»†é›†æˆæŒ‡å—:
```bash
cat ai_toolkit_integration/INTEGRATION_GUIDE.md
```

## ğŸ“– å‚è€ƒèµ„æ–™

### è®ºæ–‡

```bibtex
@article{xi2024coat,
  title={COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training},
  author={Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song},
  journal={arXiv preprint arXiv:2410.19313},
  year={2024}
}
```

### ç›¸å…³èµ„æº

- [COATè®ºæ–‡ä¸»é¡µ](https://nvlabs.github.io/COAT/)
- [ai-toolkitä»“åº“](https://github.com/ostris/ai-toolkit)
- [FLUX.1æ¨¡å‹](https://huggingface.co/black-forest-labs/FLUX.1-dev)
- [PyTorch FP8æ–‡æ¡£](https://pytorch.org/docs/stable/tensor_attributes.html#torch-tensor-dtypes)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

### å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒæ›´å¤šFP8æ ¼å¼ (FP8 E5M2, FP4)
- [ ] è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- [ ] åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- [ ] æ›´å¤šæ¨¡å‹æ”¯æŒ (SD3, Stable Diffusion XL)
- [ ] WebUIç•Œé¢

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

- NVIDIA Researchå›¢é˜Ÿçš„COATè®ºæ–‡
- ai-toolkité¡¹ç›®çš„å¼€å‘è€…
- Black Forest Labsçš„FLUXæ¨¡å‹
- PyTorchå’ŒHuggingFaceç¤¾åŒº

---

**Happy Training! ğŸ‰**

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹[é›†æˆæŒ‡å—](ai_toolkit_integration/INTEGRATION_GUIDE.md)æˆ–æäº¤Issueã€‚

