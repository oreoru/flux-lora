
# COAT FP8æ¿€æ´»é‡åŒ–é›†æˆ
import sys
from pathlib import Path

coat_path = Path(__file__).parent.parent / "coat_implementation"
sys.path.insert(0, str(coat_path))

from coat_implementation import replace_linear_with_fp8, FP8PrecisionFlow

class COATEnhancedTrainer:
    """å¢å¼ºçš„è®­ç»ƒå™¨ï¼Œé›†æˆCOATä¼˜åŒ–"""
    
    def __init__(self, config):
        self.config = config
        self.coat_enabled = config.get('coat', {}).get('enabled', False)
        self.precision_flow = None
        
        if self.coat_enabled:
            print("ğŸš€ COATä¼˜åŒ–å·²å¯ç”¨!")
            self.precision_flow = FP8PrecisionFlow()
    
    def prepare_model(self, model):
        """å‡†å¤‡æ¨¡å‹"""
        if self.coat_enabled:
            activation_config = self.config.get('coat', {}).get('activation', {})
            if activation_config.get('use_fp8', False):
                print("ğŸ”„ åº”ç”¨FP8æ¿€æ´»é‡åŒ–...")
                model = replace_linear_with_fp8(model, recursive=True)
        
        return model
    
    def training_step(self, model, batch, optimizer, step):
        """è®­ç»ƒæ­¥éª¤"""
        
        # è®°å½•å†…å­˜
        if self.coat_enabled and self.config.get('coat', {}).get('memory', {}).get('log_memory_stats', False):
            self._log_memory(step, "before_forward")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**batch)
        loss = outputs.loss if hasattr(outputs, 'loss') else outputs
        
        if self.coat_enabled:
            self._log_memory(step, "after_forward")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        if self.coat_enabled:
            self._log_memory(step, "after_backward")
        
        # æ¢¯åº¦è£å‰ª
        if 'gradient_clipping' in self.config.get('train', {}):
            torch.nn.utils.clip_grad_norm_(
                model.parameters(), 
                self.config['train']['gradient_clipping']
            )
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        optimizer.step()
        
        if self.coat_enabled:
            self._log_memory(step, "after_optimizer_step")
        
        return {'loss': loss.item()}
    
    def _log_memory(self, step, phase):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        import torch
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š [{phase}] Step {step}: Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
