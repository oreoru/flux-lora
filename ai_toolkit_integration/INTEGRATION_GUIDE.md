
# COAT + ai-toolkit é›†æˆæŒ‡å—

## 1. å®‰è£…ä¾èµ–

```bash
pip install torch>=2.1.0  # éœ€è¦æ”¯æŒFP8çš„PyTorchç‰ˆæœ¬
pip install transformers diffusers accelerate
```

## 2. ç›®å½•ç»“æ„

ç¡®ä¿ç›®å½•ç»“æ„å¦‚ä¸‹:
```
flux lora/
â”œâ”€â”€ coat_implementation/          # COATå®ç°ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fp8_optimizer.py
â”‚   â”œâ”€â”€ fp8_activation.py
â”‚   â””â”€â”€ coat_trainer.py
â”œâ”€â”€ ai-toolkit/                   # ai-toolkitä»“åº“
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ai_toolkit_integration/       # é›†æˆä»£ç 
â”‚   â”œâ”€â”€ coat_config.yaml
â”‚   â””â”€â”€ integrate_coat.py
â””â”€â”€ datasets/
    â””â”€â”€ clothing/                 # æœè£…å›¾ç‰‡æ•°æ®é›†
        â”œâ”€â”€ image1.jpg
        â”œâ”€â”€ image1.txt
        â”œâ”€â”€ image2.jpg
        â”œâ”€â”€ image2.txt
        â””â”€â”€ ...
```

## 3. ä¿®æ”¹ai-toolkitä»£ç 

### æ–¹æ³•A: æ‰‹åŠ¨é›†æˆ (æ¨è)

#### 3.1 ä¿®æ”¹ä¼˜åŒ–å™¨åˆ›å»ºä»£ç 

åœ¨ `ai-toolkit/toolkit/optimizers.py` (æˆ–ç›¸åº”æ–‡ä»¶) ä¸­æ·»åŠ :

```python
# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ 
import sys
from pathlib import Path
coat_path = Path(__file__).parent.parent.parent / "coat_implementation"
sys.path.insert(0, str(coat_path))

from coat_implementation import FP8AdamW, FP8QuantizationConfig

# åœ¨create_optimizerå‡½æ•°ä¸­æ·»åŠ 
def get_optimizer(config, parameters):
    optimizer_type = config.get('train', {}).get('optimizer', 'adamw')
    lr = config.get('train', {}).get('lr', 1e-4)
    
    if optimizer_type == "coat_fp8_adamw":
        coat_config = config.get('coat', {}).get('optimizer', {})
        fp8_config = FP8QuantizationConfig(
            use_fp8_m1=coat_config.get('use_fp8', True),
            use_fp8_m2=coat_config.get('use_fp8', True),
            m1_format=coat_config.get('m1_format', 'e4m3'),
            m2_format=coat_config.get('m2_format', 'e4m3'),
            block_size=coat_config.get('block_size', 128),
            use_dynamic_range_expansion=coat_config.get('use_dynamic_range_expansion', True)
        )
        
        return FP8AdamW(parameters, lr=lr, fp8_config=fp8_config)
    
    # åŸæœ‰é€»è¾‘...
```

#### 3.2 ä¿®æ”¹è®­ç»ƒå™¨ä»£ç 

åœ¨ `ai-toolkit/toolkit/trainer.py` (æˆ–ç›¸åº”æ–‡ä»¶) ä¸­:

```python
# å¯¼å…¥COATæ¨¡å—
from coat_implementation import replace_linear_with_fp8

class Trainer:
    def setup(self):
        # åœ¨æ¨¡å‹å‡†å¤‡é˜¶æ®µ
        if self.config.get('coat', {}).get('enabled', False):
            if self.config.get('coat', {}).get('activation', {}).get('use_fp8', False):
                print("ğŸ”„ åº”ç”¨COAT FP8æ¿€æ´»é‡åŒ–...")
                self.model = replace_linear_with_fp8(self.model)
```

### æ–¹æ³•B: ä½¿ç”¨Monkey Patching

åˆ›å»º `train_with_coat.py`:

```python
import sys
sys.path.insert(0, 'coat_implementation')

# å¯¼å…¥ai-toolkit
from ai_toolkit import train

# å¯¼å…¥COAT
from coat_implementation import COATTrainer, COATConfig

# Monkey patch ai-toolkitçš„è®­ç»ƒé€»è¾‘
original_create_optimizer = train.create_optimizer

def coat_create_optimizer(config, parameters):
    if config.get('train', {}).get('optimizer') == 'coat_fp8_adamw':
        coat_trainer = COATTrainer(COATConfig())
        return coat_trainer.create_optimizer(parameters, lr=config.get('train', {}).get('lr', 1e-4))
    return original_create_optimizer(config, parameters)

train.create_optimizer = coat_create_optimizer

# è¿è¡Œè®­ç»ƒ
if __name__ == "__main__":
    train.main()
```

## 4. å‡†å¤‡æ•°æ®é›†

### 4.1 æ•°æ®é›†æ ¼å¼

æœè£…å›¾ç‰‡æ•°æ®é›†åº”è¯¥åŒ…å«:
- å›¾ç‰‡æ–‡ä»¶: `.jpg`, `.jpeg`, `.png`
- æ ‡æ³¨æ–‡ä»¶: ä¸å›¾ç‰‡åŒåçš„ `.txt` æ–‡ä»¶

ç¤ºä¾‹:
```
datasets/clothing/
â”œâ”€â”€ dress_001.jpg
â”œâ”€â”€ dress_001.txt      # å†…å®¹: "a beautiful red [trigger] dress with floral patterns"
â”œâ”€â”€ jacket_002.jpg
â”œâ”€â”€ jacket_002.txt     # å†…å®¹: "a leather [trigger] jacket, black color"
â””â”€â”€ ...
```

### 4.2 æ ‡æ³¨å»ºè®®

- ä½¿ç”¨æè¿°æ€§æ ‡æ³¨: "a [trigger] dress with floral patterns, high quality"
- åŒ…å«è§¦å‘è¯: `[trigger]` ä¼šè¢«é…ç½®ä¸­çš„ `trigger_word` æ›¿æ¢
- æè¿°é¢œè‰²ã€æè´¨ã€é£æ ¼ç­‰ç»†èŠ‚

## 5. è¿è¡Œè®­ç»ƒ

```bash
cd ai-toolkit
python run.py ../ai_toolkit_integration/coat_config.yaml
```

æˆ–ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬:
```bash
python train_with_coat.py --config ../ai_toolkit_integration/coat_config.yaml
```

## 6. æ€§èƒ½å¯¹æ¯”

COATé¢„æœŸæ”¶ç›Š:
- âœ… å†…å­˜ä½¿ç”¨å‡å°‘ ~1.54x
- âœ… è®­ç»ƒé€Ÿåº¦æå‡ ~1.43x
- âœ… å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch size (æå‡åˆ°2-4å€)
- âœ… æ¨¡å‹ç²¾åº¦æ— æŸå¤±

### ç›‘æ§å†…å­˜ä½¿ç”¨

è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨æ‰“å°å†…å­˜ç»Ÿè®¡:
```
ğŸ“Š [before_forward] Step 100: Allocated: 8.23GB, Reserved: 10.50GB
ğŸ“Š [after_forward] Step 100: Allocated: 12.45GB, Reserved: 14.00GB
ğŸ“Š [after_backward] Step 100: Allocated: 10.67GB, Reserved: 14.00GB
ğŸ“Š [after_optimizer_step] Step 100: Allocated: 8.23GB, Reserved: 14.00GB
```

## 7. å¸¸è§é—®é¢˜

### Q: PyTorchä¸æ”¯æŒFP8æ€ä¹ˆåŠ?

A: ä»£ç ä¼šè‡ªåŠ¨é™çº§åˆ°bfloat16ã€‚è™½ç„¶å†…å­˜èŠ‚çœæ•ˆæœä¼šæ‰“æŠ˜æ‰£ï¼Œä½†ä»ç„¶å¯ä»¥è¿è¡Œã€‚å»ºè®®ä½¿ç”¨PyTorch 2.1+å’ŒCUDA 12+ã€‚

### Q: è®­ç»ƒé€Ÿåº¦æ²¡æœ‰æå‡?

A: ç¡®ä¿:
1. ä½¿ç”¨æ”¯æŒFP8çš„GPU (å¦‚H100, L40S)
2. CUDAç‰ˆæœ¬ >= 12.0
3. batch_sizeè¶³å¤Ÿå¤§ä»¥åˆ©ç”¨å†…å­˜èŠ‚çœ

### Q: ç²¾åº¦æŸå¤±?

A: COATè®¾è®¡ä¸ºæ— æŸåŠ é€Ÿã€‚å¦‚æœè§‚å¯Ÿåˆ°ç²¾åº¦æŸå¤±ï¼Œå°è¯•:
1. è°ƒæ•´ `block_size` (é»˜è®¤128)
2. ä½¿ç”¨ `e5m2` æ ¼å¼ä»£æ›¿ `e4m3`
3. ç¦ç”¨åŠ¨æ€èŒƒå›´æ‰©å±• (ä¸æ¨è)

## 8. è¿›é˜¶é…ç½®

### åªè®­ç»ƒç‰¹å®šå±‚

```yaml
network:
  type: "lora"
  linear: 16
  linear_alpha: 16
  network_kwargs:
    only_if_contains:
      - "transformer.single_transformer_blocks.7.proj_out"
      - "transformer.single_transformer_blocks.20.proj_out"
```

### è°ƒæ•´FP8æ ¼å¼

```yaml
coat:
  optimizer:
    m1_format: "e4m3"  # ä¸€é˜¶åŠ¨é‡: e4m3 (æ¨è) æˆ– e5m2
    m2_format: "e5m2"  # äºŒé˜¶åŠ¨é‡: e4m3 æˆ– e5m2
```

### è‡ªå®šä¹‰é‡åŒ–ç²’åº¦

```yaml
coat:
  activation:
    linear_granularity: "per_tensor"    # per_tensor æˆ– per_group
    nonlinear_granularity: "per_group"  
    group_size: 256                     # å¢å¤§ä»¥å‡å°‘å¼€é”€
```

## 9. å¼•ç”¨

å¦‚æœä½¿ç”¨COATæ–¹æ³•ï¼Œè¯·å¼•ç”¨åŸè®ºæ–‡:
```bibtex
@article{xi2024coat,
  title={COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training},
  author={Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song},
  journal={arXiv preprint arXiv:2410.19313},
  year={2024}
}
```
