#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPU æ˜¾å­˜ç›‘æ§å·¥å…·
å®æ—¶æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
"""

import sys
import torch
import time
import os

# ç¡®ä¿è¾“å‡ºç«‹å³æ˜¾ç¤º
sys.stdout.reconfigure(encoding='utf-8') if hasattr(sys.stdout, 'reconfigure') else None

def format_bytes(bytes_value):
    """æ ¼å¼åŒ–å­—èŠ‚æ•°"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_value < 1024.0:
            return f"{bytes_value:.2f} {unit}"
        bytes_value /= 1024.0
    return f"{bytes_value:.2f} PB"

def print_gpu_info():
    """æ‰“å°GPUä¿¡æ¯"""
    print("\n" + "="*60)
    print("  GPU æ˜¾å­˜ç›‘æ§")
    print("="*60)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼")
        return False
    
    device_count = torch.cuda.device_count()
    print(f"\næ£€æµ‹åˆ° {device_count} ä¸ª GPU è®¾å¤‡\n")
    
    for i in range(device_count):
        print(f"â”â”â” GPU {i}: {torch.cuda.get_device_name(i)} â”â”â”")
        
        # è·å–æ˜¾å­˜ä¿¡æ¯
        total_memory = torch.cuda.get_device_properties(i).total_memory
        reserved_memory = torch.cuda.memory_reserved(i)
        allocated_memory = torch.cuda.memory_allocated(i)
        free_memory = reserved_memory - allocated_memory
        
        print(f"  æ€»æ˜¾å­˜:     {format_bytes(total_memory)}")
        print(f"  å·²é¢„ç•™:     {format_bytes(reserved_memory)} ({reserved_memory/total_memory*100:.1f}%)")
        print(f"  å·²åˆ†é…:     {format_bytes(allocated_memory)} ({allocated_memory/total_memory*100:.1f}%)")
        print(f"  æœªåˆ†é…:     {format_bytes(free_memory)}")
        print(f"  ç³»ç»Ÿç©ºé—²:   {format_bytes(total_memory - reserved_memory)} ({(total_memory-reserved_memory)/total_memory*100:.1f}%)")
        
        # æ˜¾å­˜ä½¿ç”¨ç‡
        usage_percent = (allocated_memory / total_memory) * 100
        
        # è¿›åº¦æ¡
        bar_length = 40
        filled_length = int(bar_length * usage_percent / 100)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        print(f"\n  ä½¿ç”¨ç‡: [{bar}] {usage_percent:.1f}%")
        
        # è­¦å‘Š
        if usage_percent > 90:
            print("  âš ï¸  è­¦å‘Š: æ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡ 90%ï¼Œå¯èƒ½å³å°†è€—å°½ï¼")
        elif usage_percent > 80:
            print("  âš ï¸  æ³¨æ„: æ˜¾å­˜ä½¿ç”¨ç‡è¾ƒé«˜")
        elif usage_percent < 10:
            print("  âœ… æ˜¾å­˜å……è¶³")
        
        # CUDA èƒ½åŠ›
        capability = torch.cuda.get_device_capability(i)
        print(f"\n  CUDA èƒ½åŠ›: {capability[0]}.{capability[1]}")
        
        # FP8 æ”¯æŒ
        if capability[0] >= 9 or (capability[0] == 8 and capability[1] >= 9):
            print("  âœ… æ”¯æŒç¡¬ä»¶ FP8 åŠ é€Ÿ")
        else:
            print("  âš ï¸  ä¸æ”¯æŒç¡¬ä»¶ FP8 (ä½¿ç”¨è½¯ä»¶æ¨¡æ‹Ÿ)")
        
        print()
    
    return True

def continuous_monitor(interval=2):
    """è¿ç»­ç›‘æ§æ¨¡å¼"""
    try:
        while True:
            os.system('cls' if os.name == 'nt' else 'clear')
            print_gpu_info()
            print(f"\nâ±ï¸  æ¯ {interval} ç§’æ›´æ–°ä¸€æ¬¡ (æŒ‰ Ctrl+C é€€å‡º)")
            time.sleep(interval)
    except KeyboardInterrupt:
        print("\n\nâœ… ç›‘æ§å·²åœæ­¢")

def main():
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--monitor':
        print("ğŸ” å¯åŠ¨è¿ç»­ç›‘æ§æ¨¡å¼...")
        continuous_monitor()
    else:
        print_gpu_info()
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ 'python check_gpu_memory.py --monitor' å¯åŠ¨è¿ç»­ç›‘æ§\n")

if __name__ == "__main__":
    main()

