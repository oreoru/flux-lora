#!/bin/bash
# COAT + FLUX LoRA è®­ç»ƒå¯åŠ¨è„šæœ¬ - A100 40GB Linuxç‰ˆ

echo "============================================"
echo "   COAT + FLUX LoRA è®­ç»ƒå¯åŠ¨å™¨"
echo "   A100 40GB ä¼˜åŒ–ç‰ˆ - Linuxç³»ç»Ÿ"
echo "============================================"
echo ""

# è®¾ç½®HuggingFaceé•œåƒï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
# export HF_ENDPOINT=https://hf-mirror.com
# echo "âœ… å·²è®¾ç½® HuggingFace é•œåƒ"

# è®¾ç½®CUDAå†…å­˜ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo "âœ… å·²å¯ç”¨ CUDA å†…å­˜ä¼˜åŒ–"

# è®¾ç½®å¤šçº¿ç¨‹
export OMP_NUM_THREADS=8
echo "âœ… å·²è®¾ç½® OMP çº¿ç¨‹æ•°: 8"

# æ£€æµ‹ CUDA
echo ""
echo "ğŸ” æ£€æµ‹ CUDA ç¯å¢ƒ..."
python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

echo ""
echo "ğŸš€ å¼€å§‹è®­ç»ƒ..."
echo ""

# å¯åŠ¨è®­ç»ƒ
python3 train_flux_lora_with_coat.py ai_toolkit_integration/coat_config_a100.yaml

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… è®­ç»ƒå®Œæˆï¼"
    echo "ğŸ“ è¾“å‡ºç›®å½•: output/flux_lora_clothing_coat_a100/"
else
    echo ""
    echo "âŒ è®­ç»ƒå‡ºé”™ï¼"
    exit 1
fi

