"""
æ‰¾åˆ°é€‚åˆä½ GPUçš„æœ€ä½³batch size
é’ˆå¯¹ RTX 4060 Laptop (8GB)
"""

import torch
import sys

print("="*60)
print("å¯»æ‰¾æœ€ä½³Batch Size")
print("="*60)

# æ£€æŸ¥CUDA
if not torch.cuda.is_available():
    print("\nâŒ CUDAä¸å¯ç”¨ï¼")
    print("è¯·å…ˆå®‰è£…CUDAç‰ˆPyTorch:")
    print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    sys.exit(1)

print(f"\nGPU: {torch.cuda.get_device_name(0)}")
print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
print(f"PyTorch: {torch.__version__}")
print()

def test_batch_size(bs, use_fp8=False):
    """æµ‹è¯•æŒ‡å®šbatch sizeæ˜¯å¦ä¼šOOM"""
    try:
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # æ¨¡æ‹ŸFLUXè®­ç»ƒçš„æ˜¾å­˜å ç”¨
        # ä½¿ç”¨bfloat16æ¨¡æ‹Ÿæ ‡å‡†è®­ç»ƒ
        dtype = torch.bfloat16
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆæ¥è¿‘FLUXçš„å®é™…ç»´åº¦ï¼‰
        x = torch.randn(bs, 4, 128, 128, device='cuda', dtype=dtype)
        
        # æ¨¡æ‹Ÿå¤šå±‚å·ç§¯ï¼ˆç±»ä¼¼transformerå—ï¼‰
        conv1 = torch.nn.Conv2d(4, 512, 3, padding=1, device='cuda', dtype=dtype)
        conv2 = torch.nn.Conv2d(512, 512, 3, padding=1, device='cuda', dtype=dtype)
        conv3 = torch.nn.Conv2d(512, 4, 3, padding=1, device='cuda', dtype=dtype)
        
        # å‰å‘ä¼ æ’­
        y = conv1(x)
        y = torch.nn.functional.gelu(y)
        y = conv2(y)
        y = torch.nn.functional.gelu(y)
        y = conv3(y)
        
        # åå‘ä¼ æ’­
        loss = y.mean()
        loss.backward()
        
        peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        print(f"âœ… Batch size {bs:2d}: å³°å€¼æ˜¾å­˜ {peak_memory:.2f} GB")
        
        # æ¸…ç†
        del x, y, loss, conv1, conv2, conv3
        torch.cuda.empty_cache()
        return True, peak_memory
        
    except RuntimeError as e:
        if 'out of memory' in str(e):
            print(f"âŒ Batch size {bs:2d}: OOM (æ˜¾å­˜ä¸è¶³)")
            torch.cuda.empty_cache()
            return False, 0
        raise e

# æµ‹è¯•ä¸åŒbatch size
print("æµ‹è¯•æ ‡å‡†è®­ç»ƒï¼ˆBF16ï¼‰:")
print("-" * 60)

max_working_bs = 1
best_memory = 0

for bs in [1, 2, 3, 4, 6, 8]:
    success, memory = test_batch_size(bs)
    if success:
        max_working_bs = bs
        best_memory = memory
    else:
        break

print()
print("="*60)
print("å»ºè®®é…ç½®")
print("="*60)

if max_working_bs == 1:
    print("\nâš ï¸  æ˜¾å­˜è¾ƒç´§å¼ ï¼Œå»ºè®®:")
    print(f"   batch_size: 1")
    print(f"   gradient_accumulation_steps: 16  # ç­‰æ•ˆbatch size = 16")
    print(f"   resolution: [512, 768]  # ä½¿ç”¨è¾ƒå°åˆ†è¾¨ç‡")
    print(f"   LoRA rank: 8  # å‡å°rank")
    print(f"   gradient_checkpointing: true  # å¿…é¡»å¯ç”¨")
elif max_working_bs <= 4:
    print(f"\næ¨èé…ç½®ï¼ˆé€‚ä¸­ï¼‰:")
    print(f"   batch_size: {max_working_bs}")
    print(f"   gradient_accumulation_steps: {16 // max_working_bs}  # ç­‰æ•ˆbatch size = 16")
    print(f"   resolution: [768, 1024]")
    print(f"   LoRA rank: 16")
else:
    print(f"\næ¨èé…ç½®ï¼ˆæ˜¾å­˜å……è¶³ï¼‰:")
    print(f"   batch_size: {max_working_bs}")
    print(f"   gradient_accumulation_steps: {16 // max_working_bs}  # ç­‰æ•ˆbatch size = 16")
    print(f"   resolution: [1024, 1024]")
    print(f"   LoRA rank: 32")

print()
print("ğŸ’¡ æç¤º:")
print("   - ä½¿ç”¨COATå¯ä»¥è¿›ä¸€æ­¥æå‡batch size 2-4å€")
print("   - å¯ç”¨ gradient_checkpointing å¯ä»¥èŠ‚çœæ›´å¤šæ˜¾å­˜")
print("   - cache_latents_to_disk å¯ä»¥èŠ‚çœVAEç¼–ç çš„æ˜¾å­˜")

print()
print("YAMLé…ç½®ç¤ºä¾‹:")
print("-" * 60)
print(f"""
train:
  batch_size: {max_working_bs}
  gradient_accumulation_steps: {16 // max_working_bs}
  gradient_checkpointing: true
  dtype: bf16

datasets:
  resolution: [768, 1024]
  cache_latents_to_disk: true

network:
  linear: 16
  linear_alpha: 16

coat:
  enabled: true          # å¯ç”¨COATå¯ä»¥æå‡batch size
  optimizer:
    use_fp8: true
  activation:
    use_fp8: true
""")

print("="*60)
print(f"âœ… æµ‹è¯•å®Œæˆï¼å»ºè®®ä» batch_size={max_working_bs} å¼€å§‹")
print("="*60)


