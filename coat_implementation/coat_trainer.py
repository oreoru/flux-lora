"""
COATè®­ç»ƒå™¨é›†æˆæ¨¡å—
ç”¨äºŽai-toolkitçš„FLUX LoRAè®­ç»ƒ

ä½¿ç”¨æ–¹æ³•:
1. åœ¨ai-toolkitçš„è®­ç»ƒé…ç½®ä¸­å¯ç”¨COAT
2. è‡ªåŠ¨åº”ç”¨FP8ä¼˜åŒ–å™¨å’Œæ¿€æ´»é‡åŒ–
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any
import time
from dataclasses import dataclass

from .fp8_optimizer import FP8AdamW, FP8QuantizationConfig
from .fp8_activation import (
    FP8ActivationQuantizer, 
    FP8PrecisionFlow,
    replace_linear_with_fp8
)


@dataclass
class COATConfig:
    """COATè®­ç»ƒé…ç½®"""
    
    # ä¼˜åŒ–å™¨çŠ¶æ€é‡åŒ–
    use_fp8_optimizer: bool = True
    optimizer_m1_format: str = 'e4m3'  # ä¸€é˜¶åŠ¨é‡æ ¼å¼
    optimizer_m2_format: str = 'e4m3'  # äºŒé˜¶åŠ¨é‡æ ¼å¼
    optimizer_block_size: int = 128
    use_dynamic_range_expansion: bool = True
    
    # æ¿€æ´»é‡åŒ–
    use_fp8_activation: bool = True
    activation_linear_granularity: str = 'per_tensor'
    activation_nonlinear_granularity: str = 'per_group'
    activation_group_size: int = 128
    
    # è®­ç»ƒè®¾ç½®
    enable_memory_efficient_checkpoint: bool = True
    log_memory_stats: bool = True
    
    def to_fp8_quant_config(self) -> FP8QuantizationConfig:
        """è½¬æ¢ä¸ºFP8é‡åŒ–é…ç½®"""
        return FP8QuantizationConfig(
            use_fp8_m1=self.use_fp8_optimizer,
            use_fp8_m2=self.use_fp8_optimizer,
            m1_format=self.optimizer_m1_format,
            m2_format=self.optimizer_m2_format,
            block_size=self.optimizer_block_size,
            use_dynamic_range_expansion=self.use_dynamic_range_expansion
        )


class COATTrainer:
    """
    COATè®­ç»ƒå™¨
    å°è£…äº†COATçš„æ‰€æœ‰ä¼˜åŒ–æŠ€æœ¯
    """
    
    def __init__(self, config: COATConfig):
        self.config = config
        self.precision_flow = FP8PrecisionFlow() if config.use_fp8_activation else None
        self.memory_stats = []
    
    def create_optimizer(self, 
                        model_params,
                        lr: float = 1e-4,
                        **optimizer_kwargs) -> torch.optim.Optimizer:
        """
        åˆ›å»ºä¼˜åŒ–å™¨
        
        Args:
            model_params: æ¨¡åž‹å‚æ•°
            lr: å­¦ä¹ çŽ‡
            optimizer_kwargs: å…¶ä»–ä¼˜åŒ–å™¨å‚æ•°
        
        Returns:
            ä¼˜åŒ–å™¨å®žä¾‹
        """
        if self.config.use_fp8_optimizer:
            print("ðŸš€ ä½¿ç”¨COAT FP8ä¼˜åŒ–å™¨")
            fp8_config = self.config.to_fp8_quant_config()
            return FP8AdamW(
                model_params,
                lr=lr,
                fp8_config=fp8_config,
                **optimizer_kwargs
            )
        else:
            print("ä½¿ç”¨æ ‡å‡†AdamWä¼˜åŒ–å™¨")
            return torch.optim.AdamW(model_params, lr=lr, **optimizer_kwargs)
    
    def prepare_model(self, model: nn.Module) -> nn.Module:
        """
        å‡†å¤‡æ¨¡åž‹ä»¥ä½¿ç”¨COAT
        
        Args:
            model: è¦å‡†å¤‡çš„æ¨¡åž‹
        
        Returns:
            ä¿®æ”¹åŽçš„æ¨¡åž‹
        """
        if self.config.use_fp8_activation:
            print("ðŸ”„ å°†æ¨¡åž‹çš„Linearå±‚æ›¿æ¢ä¸ºFP8ç‰ˆæœ¬...")
            model = replace_linear_with_fp8(model, recursive=True)
        
        return model
    
    def log_memory(self, step: int, phase: str = ""):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.config.log_memory_stats:
            return
        
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3
            
            self.memory_stats.append({
                'step': step,
                'phase': phase,
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'timestamp': time.time()
            })
            
            print(f"ðŸ“Š [{phase}] Step {step}: "
                  f"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
    
    def get_memory_report(self) -> Dict[str, Any]:
        """èŽ·å–å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        if not self.memory_stats:
            return {}
        
        import numpy as np
        
        allocated = [s['allocated_gb'] for s in self.memory_stats]
        reserved = [s['reserved_gb'] for s in self.memory_stats]
        
        return {
            'peak_allocated_gb': max(allocated),
            'peak_reserved_gb': max(reserved),
            'avg_allocated_gb': np.mean(allocated),
            'avg_reserved_gb': np.mean(reserved),
            'total_samples': len(self.memory_stats)
        }
    
    def training_step(self,
                     model: nn.Module,
                     batch: Dict[str, torch.Tensor],
                     optimizer: torch.optim.Optimizer,
                     step: int) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ­¥è®­ç»ƒ
        
        Args:
            model: æ¨¡åž‹
            batch: æ‰¹æ¬¡æ•°æ®
            optimizer: ä¼˜åŒ–å™¨
            step: å½“å‰æ­¥æ•°
        
        Returns:
            æŸå¤±å­—å…¸
        """
        self.log_memory(step, "before_forward")
        
        # å‰å‘ä¼ æ’­
        if self.precision_flow:
            self.precision_flow.clear_cache()
        
        outputs = model(**batch)
        loss = outputs.loss if hasattr(outputs, 'loss') else outputs
        
        self.log_memory(step, "after_forward")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        self.log_memory(step, "after_backward")
        
        # ä¼˜åŒ–å™¨æ­¥è¿›
        optimizer.step()
        
        self.log_memory(step, "after_optimizer_step")
        
        return {'loss': loss.item()}


def create_coat_trainer_for_flux_lora(
    learning_rate: float = 1e-4,
    enable_fp8_optimizer: bool = True,
    enable_fp8_activation: bool = True,
    **kwargs
) -> COATTrainer:
    """
    ä¸ºFLUX LoRAè®­ç»ƒåˆ›å»ºCOATè®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        learning_rate: å­¦ä¹ çŽ‡
        enable_fp8_optimizer: æ˜¯å¦å¯ç”¨FP8ä¼˜åŒ–å™¨
        enable_fp8_activation: æ˜¯å¦å¯ç”¨FP8æ¿€æ´»
        kwargs: å…¶ä»–COATé…ç½®å‚æ•°
    
    Returns:
        COATTrainerå®žä¾‹
    
    ç¤ºä¾‹:
        >>> trainer = create_coat_trainer_for_flux_lora(
        ...     learning_rate=1e-4,
        ...     enable_fp8_optimizer=True,
        ...     enable_fp8_activation=True
        ... )
    """
    config = COATConfig(
        use_fp8_optimizer=enable_fp8_optimizer,
        use_fp8_activation=enable_fp8_activation,
        **kwargs
    )
    
    return COATTrainer(config)

